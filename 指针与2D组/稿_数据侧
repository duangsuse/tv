C++,Java,C#; Py,Rb,JS

0编号、// \"
1+'1 字串型优先, Py 检查, 0F nuudef NaN ""; 网格单项 rrel
parseInt Number, undef+ NaN
table,%c ,alert/write/debugger

这些教程告诉你计算机只认二进制、用补码去支持含负数加法，画蛇添足去教人咋做进制换算，但他们都不会告诉你为什么得是二进制，计算机又是如何利用它的。

人类的思维有没有极限？倘若把咱的思考扩大到整个宇宙，肯定会找到一个接近无穷大的数，随着规模的扩大，肯定有些数是无顶限的；倘若数一个数只耗1微秒，要数完也是不可能的；但咱都不是火星人，
你一天吃几顿饭？有几个朋友？现实的问题是有数目的，而有顶限的现实问题，才是计算机的问题。

如果你要表示一个文本，就说是一串字吧，比如是两个 长度N=3 的列表，每位从 "abc甲乙丙" 里随便选，N能不能是无限的，不能因为内存有限；那这个 abc甲乙丙 也不真实啊，中文有三字经、百家姓、千字文，新华字典里出现了多少字也不可能就6个吧，但是人能学八门语言。不考虑它们间交流时的一致性(协议/规范)，计算机能不能无限制扩充字符集，也就是字串里每位置的可能性？

刚刚我们每位有6情况，最低需 log2(6) 位二进制去表达
之所以要用二进制，因为它是唯一每一位非此即彼的『位置记数法』(好比 if,switch)，我们知道记数法只是数值的表达，而数的实质是加减乘除比大小的这些运算 方法，可不可以说内存、闪存、磁盘磁道上，哪位有电子就是1、没电子就是0，可以吧？可不可以说电线高电平是1低电平是0？那电子和时差是不是物理对象、物理值，是不是有限的；所以计算机的核心，存储、运算、程序跳转，不仅行为是可预测的，存储规模也分型号

计算机的存储，就是把读写速度快慢、掉电易失非易失的内存和外存里  可能是01 的一堆二进制位，按照某种结构进行解释，解释的最小单位称为『原始类型』，作为计算和转换的参数也叫『值』。比如 32位一个整数 int 或者真假值 bool, 8 位一个 char, 或者一个长度跟一块独占的内存片构成的列表、一个标签跟一个内存片指针，或者尾随独占的内存片，把它依标签进行解释，它们也是值，因为只要有『计算方式』，就有相应『类型』的值。

正是因为对内存位置(指针)的暴露过于赤裸而不严谨，对组合、分支类型的存储方法不统一，对存储器和编程思维的区分太小，C 语言，是相当容易被误解的二进制语言；C 语言何须序列化？它操纵的内存就可以被存住，它甚至能直接在外存里计算、读写传感器等外设的信息共享，所以现今操作系统、文件系统 C 一家独霸，但，它不会是门好的『编程语言』，过度的自由和技巧，势必导致难懂和混乱，当你只是要把 [123]的每项+1, 为啥要在意它遍历的索引(项编号)区间、存储在什么地方、何时解除分配、乃至每个数几字节、内存地址是不是连续的。

程序代码、机器码也是这种列表和分支结构体构成的，其实计算器也算计算机，但它的程序是写死的、功能是固定的；而基于几行指令和常量数据，读写存储、进行运算程控IO，得到新数据或副作用，就是现代存储程序型 电子计算机的全貌。

数学是一套思维方法，
而计算机是思维和现实交融的那个点，既足够抽象而广泛，又步骤分明，可以实现

把一个现实中的问题，严谨地分类型、分规模去讨论，就是计算机科学

那么计算机有没有无穷大？有的，毕竟我们只是严谨化了数学运算规则而已，有一个 Infinity 不小于任何数、(-它) 不大于任何数，加减乘除对它意义不大，它真正的意义是，任何数>Infinity 恒为假，所以，在任何时间你都不可能解决一个规模为无限的问题，比如刚刚的数完宇宙里所有的星星，那不是计算机讨论的问题

现在还在对除和除以锱铢必较的人，可以考虑下 1/0 ，除数不能为0 ，但是在浮点里是可以的，因为任何数里都有无穷多个0

说程序是由常量、变量、运算符构成的。
不是，如果教的是 Ruby 这种表达式语言还算对吧 arc=2*r *PI ，但 C 文件层语法结构是很死的，也没有 REPL 这样方便的调试工具，你这么说？

用这种方法自底向上的学习语法结构无异于管中窥豹、盲人摸象，如果你想真正自由地重构程序，没对基础定义、程控、表达式语言结构有个概念是不可能的，从关键字、不能咋命名入手一门语言，一些人还真是有编译原理的心没编译原理的脑啊，咋整这么麻烦，讲半天一个完整逻辑没写出来，让你去菜市场买菜跟大妈争议起微积分的微有几种写法，还觉得啊买菜不懂微积分行不懂微机怎么写咋行，给爷整不会了都

把这堆箱子数完
把这堆箱子今天内数完

数是不是一个对每个箱子都有的动作

你可以每分钟检测电量，或者如果有办法在电量不足时执行个动作不也行吗？

然后是领域的名词集
