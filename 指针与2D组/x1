下面的内容是关于涉及到二进制的 C/C++ 编程，
大部分JS/Py的应用侧程序员 可作为强基的知识点去了解，
毕竟你保不准要和C编写的接口交互呢，
我会尽量以大家能理解的顺序去细讲，希望你能坚持多刷几遍哦
WASM也越来越多用于浏览器里，可以轻松移植处理工具，
制造基于 SDL,FluidSynth,VOSK 等AI音视频应用、
高性能做到普通网页完全想不到的功能，
如果只是靠照猫画虎，
不懂C数据类型的区别，写JS的你敢做WASM EM_ASM() 应用吗？
比如 xch-ceb.github.io/xch-web/ ，就是移植了一个 Rust 控制台应用，
只需交接两边的字符串，就能直接从浏览器调用C兼容的程序、
在浏览器端执行 ls, cat 等命令
你可能看到谭某强一流的教程，
这些教程告诉你
表达式内执行顺序与等号,赋值操作的值，
还有计算机只认二进制、用补码去支持含负数加法，
还画蛇添足的教人咋做进制换算，
但他们都不会告诉你为什么得是二进制，
计算机又是如何利用它的。
让我们从计算机的来源谈起吧。
人类的思维有没有极限？
倘若把咱的思考扩大到整个宇宙，肯定会找到一个接近无穷大的数，
随着规模的扩大，肯定有些数是无顶限的；
倘若数一个数只耗1微秒，要数完也是不可能的；
但咱都不是火星人，
你
一天吃几顿饭？有几个朋友？
现实的问题是有数目的，而有顶限的现实问题，才是计算机的问题。
如果你要表示一个文本，
就说是一串字吧，
比如是两个 长度N=3 的列表，每位从 "abc甲乙丙" 里随便选，
N能不能是无限的，不能因为内存有限；
那这个 abc甲乙丙 也不真实啊，
中文有三字经、百家姓、千字文，
新华字典里出现了多少字也不可能就6个吧，
但是人能学八门语言。不考虑它们间交流时的一致性(协议/规范)，
计算机能不能无限制扩充字符集，也就是字串里每位置的可能性？
扯点有趣的，
外星人来地球，靠一根火柴杆，怎么把我们的所有文化记录带走？
我们可不可以把这些东西表达成一大串二进制？
如果浮点数是无限的，只需在它 a% 的长度划道杠，而a编码了这一串二进制；
是不是可以说a这个巨长的小数等价人类历史和文化？
但是计算或测量都是有精度限制的，这只是理论物理；
我们刚才强调，
计算机的问题是现实而严谨(即限制明确)的
刚刚谈的不涉及编码，
编码是字符列表和字节列表间的等价关系，
一串长文本是可以压缩，
也有不同字符集大小的，
只有文件才有编码的区别；而在它的字形显示在屏幕上之前，都只是一串数字编号而已
刚刚我们每位有6情况，最低需 log2(6) 位二进制去表达
之所以要用二进制，因为它是唯一每一位非此即彼的『位置记数法』(好比 if,switch)，
我们知道记数法只是数值的表达，而数的实质是「加减乘除比大小」的这些运算 方法，
可不可以说内存、闪存、磁盘磁道上，
哪位有电子就是1、没电子就是0，可以吧？可不可以说电线高电平是1低电平是0？
那电子和时差是不
是物理对象、物理值，是不是有限的；
所以计算机的核心，存储、运算、程序跳转，不仅行为是可预测的，存储规模也分型号
如果没有这种对每个1 的作用锱铢必较的心态，很难利用二进制编程，
因为计算机的世界只有0和1 ，0就是错1就是对，
没有一个缓冲区让你知道错在哪；
C里的许多片段许多陷阱，没有调试器让你知道是从哪一步开始错，
一旦执行，成败在此一举，有时候错了它也能正常运行(检错,如索引越界耗CPU)，
但这种对处理过程迷迷糊糊弄不清楚的程序，
你会让它运行在日访问千万的服务器上吗？
所以
如果你想用不断改参数，重试的方法写C程序，
我建议还是先学了JS这些调试友好、便于观察的语言再来吧
虽然一些教程对这些细节分割不合适，
但了解它们的重要性并不夸张
如果以一堆数字表示字符串，或者说文本；显然有个长度，也就是说在何处终止吧；
C 选择了 '\0' NUL 字符结尾的流(无关长度)式字符串、以 argc,
argv 按参数 
分离长度信息，
而 C++ 则将字符串长度和编码、列表指针与长度组合了起来
其实编程的许多问题，例如序列化(如二进制读写)
都是对这些单项有多长的胡乱计算；不同语言、不同人有不同的写法，可能都对，
但清晰度和可复用性会有很大不同；
输入的一类参数改变了，有的人只用改一行代码就能兼容，
而有的人 要支持，得改100行，不要也得改100行；
可配置与否高下立判，更统一，是 C++ 更常用的原因
计算机的存储，就是把读写速度快慢、掉电易失非易失的内存和外存里  
可能是01 的一堆二进制位，按照某种结构进行解释，
解释的最小单位称为『原始类型』，
它作为计算和转换 的参数也叫『值』。
比如 32位一个整数 int 或者真假值 bool, 
8 位一个 char,
或者一个长度跟一块独占的内存片构成的列表、
一个标签跟一个内存片指针，
或者尾随独占的内存片，
把它依标签进行解释；它们也是值，因为只要有『计算方式』，
就有相应『类型』的值。
C 里左值意味变量，
或者存储位置；右值意味(字面)常量，
只有左值能被重赋值，
比如 i++ 就对、 1++ 就错 (除const i=0
正是因为对内存
位置(=指针)的暴露过于赤裸而不严谨，
对组合、分支类型的存储方法不统一，
对存储器和编程思维的区分太小，
C 语言，
是相当容易被误解的二进制语言；C 语言何须序列化？它操纵的内存就可以被存住，
它甚至能直接在外存里计算、读写传感器等外设的信息共享，所以现今操作系统、文件系统 C 一家独霸，
但，它不会是门好的『编程语言』，
过度的自由和技巧，势必导致难懂和混乱，
当你只是要把 [123]的每项+1, 
为啥要在意它遍历的索引(项编号)区间、存储在什么地方、何时解除分配、乃至每个数几字节、
内存地址是不是连续的(链表,..)，这些无关细节呢。
程序代码、机器码
也是这种列表和分支的结构体构成的，其实计算器也算计算机，
但它的程序是写死的、功能是固定的；
而基于几行指令和常量数据，读写存储、进行运算程控 传感器IO，
得到新数据或副作用，就是现代存储程序型 电子计算机的全貌，
无论各种操作系统还是能下载的软件，在那个时代都是不可想象的，直到程序即数据的『冯·诺依曼计算机架构』出现。
所以现在我们知道数学和编程最大的区别，
数学是一套思维方法，而计算机是思维和现实交融的那个点，
既足够抽象而广泛，又步骤分明，可以实现
把一类现实中的问题，严谨地分变量、分类型、分规模去讨论，
就是计算机科学……中接近电子科技的部分，
但这现实与统一的思想却横贯整个计算机，而数学，恰恰是不重视限制的
那么计算机有没有无穷大？有的，毕竟我们只是严谨化了数学运算规则而已，
有一个 Infinity 不小于任何数、
(-它) 不大于任何数，
加减乘除对它意义不大，它真正的意义是，
任何数>Infinity 恒为假，所以，在任何时间你都不可能解决一个规模为无限的问题，
比如刚刚的数完宇宙里所有的星星，那不是计算机讨论的问题
现在还在对除和除以锱铢必较的人，可以考虑下，除数不能为0 ，被除数可以，
但是在浮点里 1/0 
是可以的，
因为任何数里都有无穷多个0 ；我读作0除1好还是1里除0好？
说程序是由常量、变量、运算符构成的。
不是，如果教的是 Ruby 这种表达式语言还算对吧 arc=2*r *PI ，
但 C 
文件层 的语法结构是很死的，也没有 REPL 
这样方便的调试工具，你这么说？
