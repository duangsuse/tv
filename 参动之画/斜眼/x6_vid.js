document.write`<canvas id=eC style="width:100%;height:100%"><img id=imR crossOrigin src="https://p0.itc.cn/images01/20210113/549ddf9a5b30423f91ea1bfb9d2fd3bc.jpeg"><video id=vid autoplay muted>`
å¸ƒ=e=>{W=e.width=e.offsetWidth;H=e.height=e.offsetHeight;return e.getContext("2d")}
with(Math){DEG=2*PI; distd=(w,h)=>[sqrt(w*w+h*h),atan2(h,w)] }//åœˆ,è·&è§’
ç»„=(item=Array,d=new Map)=>k=>{let v=d.get(k); if(!v)d.set(k,v=item()); return v}
rn=(a,b,f)=>{for(let i=a;i<b;i++)f(i)},rnPN=(a,l,f)=>rn(a-l,a+l,f)

r=17,x0=0,y0=0
with(g=å¸ƒ(eC)){
  strokeStyle="blue"
  rect(50,66, 60,200);arc(110,266, 40,0,DEG); stroke()
  eC.onclick=ev=>{
     g.drawImage(imR,0,0,W,H);æ‰­(x0=ev.x,y0=ev.y,100,r)
  }
}
æ‰­åŠ›=x=>(1-x)**.3 //2.8
æ‰­=(X,Y,l,r)=>{
  let å¿ƒè·=ç»„(),d,dr ,p
  rnPN(Y,l, y=>rnPN(X,l,x=>{
     [d,dr]=distd(x-X,y-Y); if(d<l)å¿ƒè·(d >>0).push(p=[x,y,0]),p[2]=dr
  }))
  0&&rn(0,l, i=>{g.fillStyle=`hsl(${i/l}turn 100%50%)`; å¿ƒè·(i).forEach(([x,y])=>g.fillRect(x,y,1,1) )  })
  rn(0,l, i=>{let ç¯=å¿ƒè·(i).sort(æ¯”è¾ƒi(2)),a=ç¯.map(px.v); a.push(...a.splice(0,æ‰­åŠ›(i/l)*r )); ç¯.forEach((p,i)=>px.vEq(p,a[i]) )  } )
}
px={v:([x,y])=>g.getImageData(x,y,1,1), vEq:([x,y],v)=>g.putImageData(v,x,y) }
æ¯”è¾ƒi=i=>(a,b)=>a[i]-b[i]

navigator.mediaDevices.getUserMedia({video:1,audio:0}).then(s=>vid.srcObject=s)
vid.onplay=()=>setInterval(()=>{g.drawImage(vid,0,0,W,H); æ‰­(x0,y0,100,r) }, 1000/30)
oncontextmenu=()=>vid.play()

//ä»¥ä¸‹å‡æ˜¯Face-api æ‘˜æŠ„ï¼Œå«åŠŸèƒ½åã€äººåæ ‡ç­¾ã€è´´çœ¼å¦†
//ç»¿å¹• videoWidth/2 drawImage(this.video, 0, 0, this.width, this.height);
if(0){
    let frame = g1.getImageData(0, 0, W,H);
    for (let P=frame.data,l = P.length / 4, i = 0; i < l; i++) {
      let r = P[i * 4 + 0];
      let g = P[i * 4 + 1];
      let b = P[i * 4 + 2];
      if (g > 100 && r > 100 && b < 43)frame.data[i * 4 + 3] = 0;//RGBA
    }
    g2.putImageData(frame, 0, 0); //å…¶èƒŒ=img

//jsdelivr.com/npm/face-api.js  ALT https://cdnjs.com/libraries/human
whF={width,height}
F.matchDimensions(eC,whF) //resize,forSize. https://github.com/justadudewhohacks/face-api.js/#loading-the-models

F.createCanvasFromMedia(vid)
ap=(o,k,...a)=>o[k].apply(o,a);
["tinyFaceDetector",...ss("Landmark68 Recognition Expressions")].map(s=>  ap(F.nets[`face${s}Net`],`loadFromUri`, "./models") )
//v AgeAndGender FaceDescriptor Mtcnn/æ˜“å¡ ssdMobilenetv1 { minConfidence: 0.5, maxResults: 3 }) { scoreThreshold: 0.2, inputSize: 608 } //CielNi
let K="Landmarks Expressions",det=add(s=>`withFace`+s,K, F.detectAllFaces(cap, new F.TinyFaceDetectorOptions()) ),
disp=F.resizeResults(det, whF)
add(s=>`draw`+s, K+" Detections",F.draw, eC,disp)

//or@ navigator. wk moz ms
navigator.mediaDevices.getUserMedia({video:$Y,facingMode:{exact:"user"},audio:$N}).then(s=>{
  v.srcObject=s//v.src=URL.createObjectURL(s)
  v.onloadedmetadata=v.play()
})//catch
v.onplay //create
  setInterval(takePhoto,1000);
takePhoto=async()=>{
if(!faceapiReady)return
draw(await detect());
}
detect=()=>faceapi.detectAllFaces(video, options).withFaceLandmarks();
// ageGenderNet è¯†åˆ«æ€§åˆ«å’Œå¹´é¾„
// faceExpressionNet è¯†åˆ«è¡¨æƒ…,å¼€å¿ƒï¼Œæ²®ä¸§ï¼Œæ™®é€š
// faceLandmark68Net è¯†åˆ«è„¸éƒ¨ç‰¹å¾ï¼ ç”¨äºmobilenetç®—æ³•
// faceLandmark68TinyNet è¯†åˆ«è„¸éƒ¨ç‰¹å¾ç”¨äºtinyç®—æ³•
// faceRecognitionNet è¯†åˆ«äººè„¸!
// ssdMobilenetv1 googleå¼€æºAIç®—æ³•é™¤åº“åŒ…å«åˆ†ç±»å’Œçº¿æ€§å›å½’
// tinyFaceDetector æ¯”Googleçš„mobilenetæ›´è½»é‡çº§ï¼Œé€Ÿåº¦æ›´å¿«ä¸€ç‚¹
// tinyYolov2 è¯†åˆ«èº«ä½“è½®å»“çš„ç®—æ³•ï¼Œä¸çŸ¥é“æ€ä¹ˆç”¨
//https://www.cnblogs.com/neozhu/p/11771148.html

await faceapi.loadModels(MODEL_URL) //https://github.com/justadudewhohacks/face-api.js/tree/master/weights
await faceapi.loadFaceDetectionModel(MODEL_URL)
F.allFaces(input, minConfidence=.8).map(fd => fd.forSize(width, height))
.forEach((fd, i) => {
  F.drawDetection(canvas, fd.detection, { withScore: true })
  .drawLandmarks(canvas, fd.landmarks, { drawLines: true })
  faceapi.euclideanDistance(fd.descriptors, it) //.sort(maxTop =(a,b)=>a-b)[0]

})
Promise.all([].map(uri => fetch(uri).blob().then(F.bufferToImage)) )//è¿™ä¸ªäººä¹±ç”¨async..

faceapi.drawDetection(g, r.detection, { withScore: false })
faceapi.draw.DrawTextField([""], result.detection.box.bottomRight)
let{ x, y, height: boxHeight } = detection.getBox()
faceapi.drawText(g,
  x,y + boxHeight,
  `${r.distance < .6 ? r.className : 'unkown'} (${result.distance})`
)
F.extractFaces(eC,[ Rect(0, 0, 100, 100)])
faceMatcher.findBestMatch
//https://segmentfault.com/a/1190000040583469  getBBox() of faces g.drawImage(imRand,...b.XYWH, 0,0,W,H)
//and lamdmark(with ssdMobilenetv1?) eyeLR=face.landmarks.positions.slice(36,42+6) . lip=48+
// cxcy=getBBox(eye[0]).center
// font = `${6*b.h}px serif`; textAlign = 'center';textBaseline = 'bottom'; fillText('ğŸ‘„', cx, cy + h);
/*@hack's orig-ver has NO CDN. ä½œè€…ä¹Ÿä½œ human
<script src="https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.6.3/dist/face-api.js"></script>
<video id=vid>
<script>D=document,F=faceapi
navigator.mediaDevices.getUserMedia({video:1,audio:0}).then(s=>vid.srcObject=s)
vid.onloadedmetadata=()=>D.body.append(
  eC=F.createCanvasFromMedia(vid))
vid.play()
F.loadFaceDetectionModel("https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.6.3/model/").then(()=>{
  for(let o of F.allFaces(vid, .8))F.drawDetection(eC, o.forSize(width, height).detection, { withScore: true })

})
</script>
vid.onloadedmetadata=()=>{D.body.append(
  eC=F.createCanvasFromMedia(vid))
  let c=new F.TinyFaceDetectorOptions({scoreThreshold: 0.1, inputSize: 640}), {width:W,height:H}=eC
  eC.onmousemove=F.detectAllFaces(vid, c).run().then(det=>{for(let o of det)F.draw.drawDetections(eC, o.forSize(W, H).detection, { withScore: true })} ) //ComposableTask
}
vid.play();
(async()=>{
await F.loadTinyFaceDetectorModel("https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.6.3/model/")

})()  F. Point Rect Box/xywh BoudingBox/xy,+-wh; fetchVideo,faceDesc ;env draw å•æ¬¡æ£€æµ‹å¯é™„is/extendè½®å»“/è¡¨æƒ… ç­‰ */
}
